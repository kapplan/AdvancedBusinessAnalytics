{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab97b63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:02.898278Z",
     "start_time": "2024-06-21T20:38:43.761135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /opt/anaconda3/lib/python3.9/site-packages (0.41.0)\n",
      "Requirement already satisfied: numba in /opt/anaconda3/lib/python3.9/site-packages (from shap) (0.58.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from shap) (1.26.1)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /opt/anaconda3/lib/python3.9/site-packages (from shap) (4.64.1)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/anaconda3/lib/python3.9/site-packages (from shap) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.9/site-packages (from shap) (1.4.4)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.9/site-packages (from shap) (2.0.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/anaconda3/lib/python3.9/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (from shap) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.9/site-packages (from shap) (1.11.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>20.9->shap) (3.0.9)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/anaconda3/lib/python3.9/site-packages (from numba->shap) (0.41.0)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.25.2-cp39-cp39-macosx_10_9_x86_64.whl (20.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->shap) (2022.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.1\n",
      "    Uninstalling numpy-1.26.1:\n",
      "      Successfully uninstalled numpy-1.26.1\n",
      "  Rolling back uninstall of numpy\n",
      "  Moving to /opt/anaconda3/bin/f2py\n",
      "   from /private/var/folders/vb/kcvn0bhj6yl2ftbtxj09bfjc0000gn/T/pip-uninstall-ya77ahr7/f2py\n",
      "  Moving to /opt/anaconda3/lib/python3.9/site-packages/numpy-1.26.1.dist-info/\n",
      "   from /opt/anaconda3/lib/python3.9/site-packages/~umpy-1.26.1.dist-info\n",
      "  Moving to /opt/anaconda3/lib/python3.9/site-packages/numpy/\n",
      "   from /opt/anaconda3/lib/python3.9/site-packages/~umpy\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.9/site-packages (from seaborn) (1.11.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/anaconda3/lib/python3.9/site-packages (from seaborn) (1.26.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/anaconda3/lib/python3.9/site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (5.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.2->seaborn) (3.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Collecting missingno\n",
      "  Downloading missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.9/site-packages (from missingno) (3.8.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.9/site-packages (from missingno) (1.11.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from missingno) (1.26.1)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.9/site-packages (from missingno) (0.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (1.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (4.25.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (5.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->missingno) (3.0.9)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from seaborn->missingno) (1.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->missingno) (3.8.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.23->seaborn->missingno) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n",
      "Installing collected packages: missingno\n",
      "Successfully installed missingno-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1743463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:02.915399Z",
     "start_time": "2024-06-21T20:39:02.912217Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.25 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vb/kcvn0bhj6yl2ftbtxj09bfjc0000gn/T/ipykernel_8218/497711894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"As of version 0.29.0 shap only supports Python 3 (not 2)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCohorts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/shap/_explanation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mslicer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSlicer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# from ._order import Order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDimensionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/shap/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_clustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhclust_ordering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_tree_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_minimization_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhclust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapproximate_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massert_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_import_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshapley_coefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordinal_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_stderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_show_progress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_masked_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0m_ensure_critical_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m# END DO NOT MOVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnumpy_version\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Numba needs NumPy 1.25 or less\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 1.25 or less"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0cd8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:02.987835Z",
     "start_time": "2024-06-21T20:39:02.920682Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/apple/Downloads/claims_q12023.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff610e95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:08.859680Z",
     "start_time": "2024-06-21T20:39:02.959294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initial EDA: Check for missing values\n",
    "print(\"Initial Missing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Replace '?' with np.nan in the 'property_damage' column, 'None' in 'emg_services_notified' column with np.nan,\n",
    "# empty spaces with np.nan too.\n",
    "data['property_damage'] = data['property_damage'].replace('?', np.nan)\n",
    "data['emg_services_notified'] = data['emg_services_notified'].replace('None', np.nan)\n",
    "data['police_report_avlbl'] = data['police_report_avlbl'].replace('', np.nan)\n",
    "\n",
    "# Generate descriptive statistics\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(data.describe(include='all'))\n",
    "\n",
    "# Convert dates from strings to datetime objects with the correct format\n",
    "data['coverage_start_date'] = pd.to_datetime(data['coverage_start_date'], format='%d.%m.%Y')\n",
    "data['claim_incurred_date'] = pd.to_datetime(data['claim_incurred_date'], format='%d.%m.%Y')\n",
    "\n",
    "# Visualizing the missingness pattern\n",
    "msno.matrix(data)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap to show correlations of missingness between different columns\n",
    "msno.heatmap(data)\n",
    "plt.show()\n",
    "\n",
    "# Bar chart to show the count of missing values in each column\n",
    "msno.bar(data)\n",
    "plt.show()\n",
    "\n",
    "# Cross Tabulation\n",
    "for col in ['cust_region', 'sum_assured_group', 'insured_sex', 'edu_lvl', 'marital_status', 'claim_type']:\n",
    "    print(pd.crosstab(data[col], data['property_damage'].isnull()))\n",
    "    print(pd.crosstab(data[col], data['emg_services_notified'].isnull()))\n",
    "    print(pd.crosstab(data[col], data['police_report_avlbl'].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b821e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:08.895084Z",
     "start_time": "2024-06-21T20:39:08.861847Z"
    }
   },
   "outputs": [],
   "source": [
    "# We assume that for 'parked car' and 'theft' incidents, the 'acc_type' should be set to 'Not Applicable' and 'Minor incident',\n",
    "# because the nature of missingness indicate that there is\n",
    "# Set 'acc_type' to 'Not Applicable' for 'theft' incidents\n",
    "data.loc[data['claim_type'] == 'theft', 'acc_type'] = 'Not Applicable'\n",
    "\n",
    "# Set 'acc_type' to 'Minor incident' for 'parked car' incidents\n",
    "data.loc[data['claim_type'] == 'parked car', 'acc_type'] = 'Minor incident'\n",
    "\n",
    "# Impute missing values with the most common value for each of the three columns\n",
    "for column in ['emg_services_notified', 'property_damage', 'police_report_avlbl']:\n",
    "    most_common_value = data[column].mode()[0]\n",
    "    data[column].fillna(most_common_value, inplace=True)\n",
    "    print(f\"Imputed missing values in '{column}' with '{most_common_value}'.\")\n",
    "\n",
    "# Check missing values count again to ensure they are filled\n",
    "print(\"\\nMissing values per column after imputation:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5c4ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:09.077257Z",
     "start_time": "2024-06-21T20:39:08.903425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Segment 'cust_age' into age groups\n",
    "data['age_group'] = pd.cut(data['cust_age'], bins=[18, 25, 35, 45, 55, 65, np.inf], labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "# Drop the 'cust_age' column if you decide it's no longer needed\n",
    "data.drop('cust_age', axis=1, inplace=True)\n",
    "\n",
    "# New features for risk analysis: Claim Frequency and Severity Metrics\n",
    "# Claim Frequency Features\n",
    "# Count total number of claims by policy_id\n",
    "data['total_claims'] = data.groupby('policy_id')['policy_id'].transform('count')\n",
    "\n",
    "# Claim Severity Features\n",
    "# Average, max, and total claim amount by policy_id\n",
    "data['avg_claim_amount'] = data.groupby('policy_id')['total_claim_amount'].transform('mean')\n",
    "data['max_claim_amount'] = data.groupby('policy_id')['total_claim_amount'].transform('max')\n",
    "data['total_claimed_amount'] = data.groupby('policy_id')['total_claim_amount'].transform('sum')\n",
    "\n",
    "# Net Contribution: The net contribution of a customer to the company is the difference between the premiums paid and the claims cost. A positive value indicates profitability, while a negative value indicates a loss\n",
    "# Calculate the duration of each policy in years\n",
    "data['policy_duration_years'] = (data['claim_incurred_date'] - data['coverage_start_date']).dt.days / 365.25\n",
    "\n",
    "# Calculate the total premiums paid by each customer\n",
    "# Assuming 'annual_prem' is the annual premium\n",
    "data['total_premiums_paid'] = data['annual_prem'] * data['policy_duration_years']\n",
    "\n",
    "# Calculate the total claims cost for each policy\n",
    "# Assuming 'total_claim_amount' is the claim amount per incident\n",
    "data['total_claims_cost'] = data.groupby('policy_id')['total_claim_amount'].transform('sum')\n",
    "\n",
    "# Calculate the net contribution for each policy\n",
    "data['net_contribution'] = data['total_premiums_paid'] - data['total_claims_cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c4af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:11.345049Z",
     "start_time": "2024-06-21T20:39:08.928915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "\n",
    "# Histogram of Customer Ages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['age_group'], bins=30, kde=True)\n",
    "plt.title('Distribution of Age Groups')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "#Average Claim amount by age\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='age_group', y='avg_claim_amount', data=data)\n",
    "plt.title('Average Claim Amount by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Average Claim Amount')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of Total Claim Amount by Customer Region\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cust_region', y='total_claimed_amount', data=data)\n",
    "plt.title('Total Claim Amount by Customer Region')\n",
    "plt.xlabel('Customer Region')\n",
    "plt.ylabel('Total Claim Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1364a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:14.071319Z",
     "start_time": "2024-06-21T20:39:11.345655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the distribution of key features\n",
    "data['time_to_claim'] = (data['claim_incurred_date'] - data['coverage_start_date']).dt.days\n",
    "\n",
    "for col in ['annual_prem', 'total_claim_amount', 'time_to_claim']:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n",
    "\n",
    "# Checking for outliers\n",
    "for col in ['annual_prem', 'total_claim_amount']:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(data[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n",
    "\n",
    "# Correlation Analysis\n",
    "correlation_matrix = data[['annual_prem', 'total_claim_amount', 'time_to_claim']].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c56cbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:14.930190Z",
     "start_time": "2024-06-21T20:39:14.082208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Profitability\n",
    "sns.histplot(data['net_contribution'], bins=20, kde=True)\n",
    "plt.title('Net Contribution Distribution')\n",
    "plt.xlabel('Net Contribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='total_premiums_paid', y='net_contribution', data=data)\n",
    "plt.title('Net Contribution vs Total Premiums Paid')\n",
    "plt.xlabel('Total Premiums Paid')\n",
    "plt.ylabel('Net Contribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d12601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:39:16.818830Z",
     "start_time": "2024-06-21T20:39:14.944147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace 'numerical_cols' and 'categorical_cols' with column names\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create transformers for numerical and categorical data\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply the preprocessor\n",
    "X_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Define the range of clusters to try\n",
    "range_n_clusters = range(2, 11)\n",
    "\n",
    "# Initialize lists to store the results of the silhouette score and inertia\n",
    "silhouette_avg_scores = []\n",
    "inertia_scores = []\n",
    "\n",
    "# Calculate silhouette score and inertia for different numbers of clusters\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Initialize the clusterer with n_clusters value and a random generator seed for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = clusterer.fit_predict(X_preprocessed)\n",
    "\n",
    "    # Silhouette score\n",
    "    silhouette_avg = silhouette_score(X_preprocessed, cluster_labels)\n",
    "    silhouette_avg_scores.append(silhouette_avg)\n",
    "\n",
    "    # Inertia\n",
    "    inertia_scores.append(clusterer.inertia_)\n",
    "\n",
    "# Plotting the silhouette scores\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_n_clusters, silhouette_avg_scores, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score For Optimal Number of Clusters')\n",
    "\n",
    "# Plotting the elbow method results\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_n_clusters, inertia_scores, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method For Optimal Number of Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3aa17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:41:04.646364Z",
     "start_time": "2024-06-21T20:41:03.542500Z"
    }
   },
   "outputs": [],
   "source": [
    "# TruncatedSVD, K-Means clustering\n",
    "# Define which columns are categorical\n",
    "categorical_cols = data.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "# Define which columns are numerical\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "# Applying the preprocessor\n",
    "X_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of components to retain 95% of the variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1  # +1 because indices start at 0\n",
    "\n",
    "# Now, reapply TruncatedSVD with the chosen number of components\n",
    "svd_95 = TruncatedSVD(n_components=n_components_95)\n",
    "X_svd_95 = svd_95.fit_transform(X_preprocessed)\n",
    "\n",
    "# Then determine the optimal number of clusters using silhouette scores\n",
    "range_n_clusters = range(2, 11)\n",
    "best_n_clusters = 0\n",
    "best_silhouette_score = -1\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = clusterer.fit_predict(X_svd)  # Using SVD-transformed data\n",
    "    silhouette_avg = silhouette_score(X_svd, cluster_labels)\n",
    "\n",
    "    # Check if this silhouette score is the best one so far\n",
    "    if silhouette_avg > best_silhouette_score:\n",
    "        best_n_clusters = n_clusters\n",
    "        best_silhouette_score = silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad02275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:44:31.378797Z",
     "start_time": "2024-06-21T20:44:29.944350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply K-Means clustering on the reduced SVD data\n",
    "kmeans = KMeans(n_clusters=best_n_clusters, init='k-means++', random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_svd_95)\n",
    "\n",
    "# Create a DataFrame for the SVD results for easy plotting\n",
    "column_names = [f'Component_{i+1}' for i in range(X_svd_95.shape[1])]\n",
    "svd_df = pd.DataFrame(X_svd_95, columns=column_names)\n",
    "svd_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Plotting the clusters using the first two SVD components\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Component_1', y='Component_2', hue='Cluster', data=svd_df, palette='viridis')\n",
    "plt.title('Cluster Visualization with TruncatedSVD Components')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the centroids for the reduced dimensionality data\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Calculate distances from each point to its cluster centroid in the reduced space\n",
    "distances = np.linalg.norm(X_svd_95 - centroids[cluster_labels, :], axis=1)\n",
    "\n",
    "# Set a threshold for outlier detection, e.g., beyond 2 standard deviations of the mean distance\n",
    "threshold = np.mean(distances) + 2.4 * np.std(distances)\n",
    "\n",
    "# Identify potential outlier indices\n",
    "outlier_indices = np.where(distances > threshold)[0]\n",
    "\n",
    "# Retrieve the original IDs of the outliers\n",
    "outlier_ids = data.iloc[outlier_indices]['policy_id'].values\n",
    "\n",
    "print(\"Potential Outlier IDs:\")\n",
    "print(outlier_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d06d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:41:39.138932Z",
     "start_time": "2024-06-21T20:41:34.761401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Second Approach to Segmentation\n",
    "# Feature Selection for Clustering\n",
    "features = data[['annual_prem', 'total_claim_amount']]  # Adjust based on our EDA\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Dimensionality Reduction using PCA\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Step 3: Finding the Optimal Number of Clusters\n",
    "wcss = []\n",
    "silhouette_coefficients = []\n",
    "for k in range(2, 11):  # Test different numbers of clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_pca)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    score = silhouette_score(features_pca, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "\n",
    "# Elbow Method Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 11), wcss, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Coefficient Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 11), silhouette_coefficients, marker='o')\n",
    "plt.title('Silhouette Coefficients')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.show()\n",
    "\n",
    "# Choose the number of clusters based on the plots and set it in the final K-means model\n",
    "optimal_clusters = 3\n",
    "\n",
    "# Step 4: Final K-means Clustering\n",
    "kmeans_final = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "data['segment_label'] = kmeans_final.fit_predict(features_pca)\n",
    "\n",
    "# Step 5: Visualization of Segments\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=data['segment_label'], palette='viridis')\n",
    "plt.title('Customer Segments after PCA')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Segment')\n",
    "plt.show()\n",
    "\n",
    "# Additional segment analysis and visualizations\n",
    "# Segment-wise Descriptive Statistics\n",
    "for i in range(optimal_clusters):\n",
    "    print(f\"\\nSegment {i} Statistics:\\n\", data[data['segment_label'] == i].describe())\n",
    "\n",
    "# Visualizing Segment-wise Average Claim Amounts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='segment_label', y='total_claim_amount', data=data, estimator=np.mean)\n",
    "plt.title('Average Total Claim Amount by Segment')\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Average Total Claim Amount')\n",
    "plt.show()\n",
    "\n",
    "# Visualizing Segment-wise Customer Age Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='segment_label', y='age_group', data=data)\n",
    "plt.title('Customer Age Group Distribution by Segment')\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Customer Age Group')\n",
    "plt.show()\n",
    "\n",
    "# Visualizing Segment-wise Annual Premium\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='segment_label', y='annual_prem', data=data, estimator=np.mean)\n",
    "plt.title('Average Annual Premium by Segment')\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Average Annual Premium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452a3e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:56:14.907289Z",
     "start_time": "2024-06-21T20:56:14.672030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Naming the outliers\n",
    "iso_forest = IsolationForest(contamination=0.01) # contamination is an estimate of the proportion of outliers\n",
    "outliers = iso_forest.fit_predict(features_pca)\n",
    "\n",
    "# Find the index of outliers\n",
    "outlier_indices = np.where(outliers == -1)[0]\n",
    "\n",
    "# Map indices to original data\n",
    "suspicious_policy_ids = data.iloc[outlier_indices]['policy_id'].values\n",
    "\n",
    "# Print suspicious policy IDs\n",
    "print(suspicious_policy_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de74675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:59:05.445345Z",
     "start_time": "2024-06-21T20:59:00.217545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identifying outliers using Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.01)  # contamination is an estimate of the proportion of outliers\n",
    "outliers = iso_forest.fit_predict(features_pca)\n",
    "\n",
    "# Find the indices of outliers\n",
    "outlier_indices = np.where(outliers == -1)[0]\n",
    "\n",
    "# Map the indices to original data to get suspicious policy IDs\n",
    "suspicious_policy_ids = data.iloc[outlier_indices]['policy_id'].values\n",
    "\n",
    "# Print suspicious policy IDs\n",
    "print(suspicious_policy_ids)\n",
    "\n",
    "# Define numerical and categorical features for preprocessing\n",
    "numerical_features = ['total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'num_vehicles_involved']\n",
    "categorical_features = ['police_report_avlbl', 'edu_lvl']\n",
    "\n",
    "# Create a preprocessor for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),  # Standardize numerical features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # One-hot encode categorical features\n",
    "    ])\n",
    "\n",
    "# Initialize Isolation Forest for anomaly detection\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "\n",
    "# Create a pipeline that preprocesses the data and then applies the Isolation Forest model\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('model', iso_forest)])\n",
    "\n",
    "# Combine numerical and categorical features for input data\n",
    "X = data[numerical_features + categorical_features]\n",
    "\n",
    "# Fit the pipeline on the data\n",
    "pipeline.fit(X)\n",
    "\n",
    "# Predict anomalies in the data\n",
    "anomalies = pipeline.predict(X)\n",
    "\n",
    "# Add the anomaly score to the original data\n",
    "data['anomaly_score'] = anomalies\n",
    "\n",
    "# Sort the anomalies and select the top 5 based on the total claim amount\n",
    "anomalies_df = data[data['anomaly_score'] == -1].sort_values('total_claim_amount', ascending=False)\n",
    "top_anomalies = anomalies_df.head(5)\n",
    "\n",
    "# Display the top anomalies with relevant details\n",
    "print(top_anomalies[\n",
    "          ['policy_id', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'police_report_avlbl',\n",
    "           'anomaly_score', 'edu_lvl']])\n",
    "\n",
    "# Transform the input data for SHAP analysis\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Generate new feature names after one-hot encoding\n",
    "new_feature_names = numerical_features.copy()\n",
    "for feature in categorical_features:\n",
    "    unique_values = data[feature].unique()\n",
    "    for value in unique_values:\n",
    "        new_feature_names.append(f\"{feature}_{value}\")\n",
    "all_feature_names = numerical_features + list(new_feature_names)\n",
    "\n",
    "# Explain the Isolation Forest model using SHAP\n",
    "explainer = shap.TreeExplainer(pipeline.named_steps['model'])\n",
    "\n",
    "# Calculate SHAP values for the transformed data\n",
    "shap_values = explainer.shap_values(X_transformed)\n",
    "\n",
    "# Plot the summary of SHAP values to show feature importance\n",
    "shap.summary_plot(shap_values, features=X_transformed, feature_names=all_feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
